import tiktoken
import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# models
from langchain_openai import ChatOpenAI
# from langchain_anthropic import ChatAnthropic
# from langchain_google import ChatGoogleGenerativeAI

MODEL_PRICES = {
    "Input" : {
        "gpt-4.1-nano": 0.1/ 1_000_000,
        "gpt-4.1-mini": 0.4 / 1_000_000,
        "o4-mini": 0.15 / 1_000_000,
    },
    "Output" : {
        "gpt-4.1-nano": 0.4 / 1_000_000,
        "gpt-4.1-mini": 1.6 / 1_000_000,
        "o4-mini": 0.6 / 1_000_000,
    }
}


def init_page():
    st.set_page_config(
        page_title="Myé–¢è¥¿å¼ChatGPT",
        page_icon="ğŸ™",)
    st.header("Myé–¢è¥¿å¼ğŸ™ChatGPTğŸ¤–")
    st.sidebar.title("è¨­å®š")

def init_messages():
    clear_botton = st.sidebar.button("Clear Chat", key="clear")
    # clear_botton ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ã€message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if clear_botton or "message_history" not in st.session_state:
        st.session_state.message_history = [
            ("system", "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚é–¢è¥¿å¼ã§è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚"),
            ]

def select_model():
    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§è¨­å®šå¯èƒ½ã™ã‚‹
    # åˆæœŸå€¤ã¯0.0, åˆ»ã¿å¹…ã¯0.1

    
    models = ["gpt-4.1-nano", "gpt-4.1-mini", "o4-mini"]
    model = st.sidebar.radio("Choose a model:", models)
    if model == "o4-mini":
        st.sidebar.write("ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ temperature ã¯ 1.0 å›ºå®šã§ã™")
        temperature = 1.0
    else:
        temperature = st.sidebar.slider(
        "Temperature:", min_value=0.0, max_value=2.0, value=0.0, step=0.1 
        )
    
    if model == "gpt-4.1-nano":
        st.session_state.model_name = "gpt-4.1-nano" #session_stateã«ä¿å­˜
        # debug
        llm = ChatOpenAI(temperature=temperature, model=st.session_state.model_name)
        st.sidebar.write(f"[DEBUG]â–¶ Using LLM: {model}, temperature={getattr(llm, 'temperature', 'N/A')}")
        return llm

    elif model == "gpt-4.1-mini":
        st.session_state.model_name = "gpt-4.1-mini"    #session_stateã«ä¿å­˜
        # debug
        llm = ChatOpenAI(temperature=temperature, model=st.session_state.model_name)
        st.sidebar.write(f"[DEBUG]â–¶ Using LLM: {model}, temperature={getattr(llm, 'temperature', 'N/A')}")
        return llm

    elif model == "o4-mini":
        st.session_state.model_name = "o4-mini"
        # debug
        llm = ChatOpenAI(temperature=1.0, model=st.session_state.model_name) # o4-miniã¯temperatureã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã€temperatureã¯æŒ‡å®šã—ãªã„
        st.sidebar.write(f"[DEBUG]â–¶ Using LLM: {model}, temperature={getattr(llm, 'temperature', 'N/A')}")
        return llm
    
    else:
        st.session_state.model_name = "gpt-4.1-nano" #session_stateã«ä¿å­˜
        return ChatOpenAI(temperature=temperature, model=st.session_state.model_name)

def init_chain():
    llm = select_model()
    st.session_state.llm = llm
    
    prompt = ChatPromptTemplate.from_messages(
        [
            *st.session_state.message_history,  # æ—¢å­˜ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’è¿½åŠ 
            ("user", "{user_input}"),
        ]
    )
    output_parser = StrOutputParser()
    chain = prompt | st.session_state.llm | output_parser
    return chain


def get_message_counts(text):
    model_name = st.session_state.model_name
    
    # Gemini ç³»ãªã‚‰ LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã«ä»»ã›ã‚‹
    if "gemini" in model_name:
        return st.session_state.llm.get_num_tokens(text)

    # GPT ç³»ã¯ tiktoken ã§ã‚«ã‚¦ãƒ³ãƒˆ
    if "gpt" in model_name:
        try:
            encoding = tiktoken.encoding_for_model(model_name)
        except KeyError:
            # æœªç™»éŒ²ãƒ¢ãƒ‡ãƒ«ãŒæ¥ãŸã‚‰æ¨™æº–ã® cl100k_base ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            encoding = tiktoken.get_encoding("cl100k_base")
    else:
        # Claude ç³»ãªã©ä»–ãƒ¢ãƒ‡ãƒ«ã¯ä»®ã« cl100k_base ã‚’ä½¿ã†
        encoding = tiktoken.get_encoding("cl100k_base")

    return len(encoding.encode(text))


def calc_and_display_costs():
    output_count = 0
    input_count = 0
    for role, message in st.session_state.message_history:
        # tiktokenã‚’ä½¿ã£ã¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        token_count = get_message_counts(message)
        if role == "ai":
            output_count += token_count
        else:
            input_count += token_count
    
    # åˆæœŸçŠ¶æ…‹ã§ System Message ã®ã¿ãŒå±¥æ­´ã«å…¥ã£ã¦ã„ã‚‹å ´åˆã¯ã¾ã APIã‚³ãƒ¼ãƒ«ãŒè¡Œã‚ã‚Œã¦ã„ãªã„
    if len(st.session_state.message_history) == 1:
        return

    input_cost = MODEL_PRICES["Input"][st.session_state.model_name] * input_count
    output_cost = MODEL_PRICES["Output"][st.session_state.model_name] * output_count
    if "gemini" in st.session_state.model_name and (input_count + output_count) > 128000:
        input_cost *= 2
        output_cost *= 2
    
    cost = output_cost + input_cost

    st.sidebar.markdown("## ã‚³ã‚¹ãƒˆ")
    st.sidebar.markdown(f"**ãƒˆãƒ¼ã‚¿ãƒ«ã‚³ã‚¹ãƒˆ: ${cost:.5f}**")
    st.sidebar.markdown(f"- å…¥åŠ›ã‚³ã‚¹ãƒˆ: ${input_cost:.5f}")
    st.sidebar.markdown(f"- å‡ºåŠ›ã‚³ã‚¹ãƒˆ: ${output_cost:.5f}")


def main():
    init_page()
    init_messages()
    chain = init_chain()

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º(ç¬¬2ç« ã‹ã‚‰å°‘ã—ä½ç½®ãŒå¤‰æ›´ã«ãªã£ã¦ã„ã‚‹ã®ã§æ³¨æ„)
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)
    
    if user_input := st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ãªï¼"):
        st.chat_message("user").markdown(user_input)

        #LLMã®è¿”ç­”ã‚’Streamingè¡¨ç¤ºã™ã‚‹
        with st.chat_message("ai"):
            response = st.write_stream(chain.stream({"user_input": user_input}))
        
        #ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        st.session_state.message_history.append(("user", user_input))
        st.session_state.message_history.append(("ai", response))

    #ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º
    calc_and_display_costs()


if __name__ == "__main__":
    main()